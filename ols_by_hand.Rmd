---
title: "Simple Linear Regression by Hand: Least Squares via Partial Derivatives"
author: "Heini"
date: "`r format(Sys.Date())`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    number_sections: true
fontsize: 11pt
geometry: margin=1in
header-includes:
  - \usepackage{amsmath}
  - \usepackage{mathtools}
  - \usepackage[makeroom]{cancel}
  - \usepackage{microtype}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)
```

# Problem statement

We observe the data points

\[
(x_i,y_i)\in\{(1,2),(2,3),(3,5),(4,4)\},\qquad n=4.
\]

We fit the simple linear regression model

\[
\widehat{y}_i = \beta_0 + \beta_1 x_i
\]

by *ordinary least squares* (OLS), i.e., we choose \((\beta_0,\beta_1)\) to minimize
the *sum of squared residuals*

\[
S(\beta_0,\beta_1)=\sum_{i=1}^n \Big(y_i-\beta_0-\beta_1x_i\Big)^2.
\]

# Step 1: Define residuals and the objective

Define residuals

\[
e_i(\beta_0,\beta_1)=y_i-\beta_0-\beta_1x_i,
\]

so that

\[
S(\beta_0,\beta_1)=\sum_{i=1}^n e_i(\beta_0,\beta_1)^2.
\]

# Step 2: Partial derivatives (explicit chain rule)

## 2.1 Partial derivative with respect to \(\beta_0\)

Start with

\[
\frac{\partial S}{\partial \beta_0}
=
\sum_{i=1}^n \frac{\partial}{\partial \beta_0}\left(e_i^2\right).
\]

Use the chain rule \(\frac{\partial}{\partial \beta_0}(e_i^2)=2e_i\frac{\partial e_i}{\partial\beta_0}\):

\[
\frac{\partial S}{\partial \beta_0}
=
\sum_{i=1}^n 2e_i\frac{\partial e_i}{\partial\beta_0}.
\]

Compute the inner derivative:

\[
e_i=y_i-\beta_0-\beta_1x_i
\quad\Longrightarrow\quad
\frac{\partial e_i}{\partial\beta_0}=-1.
\]

Therefore,

\[
\frac{\partial S}{\partial \beta_0}
=
\sum_{i=1}^n 2e_i(-1)
=
-2\sum_{i=1}^n e_i.
\]

Set the partial derivative to zero:

\[
-2\sum_{i=1}^n e_i = 0
\quad\Longleftrightarrow\quad
\sum_{i=1}^n (y_i-\beta_0-\beta_1x_i)=0.
\]

Now expand the sum *without skipping steps*:

\[
\sum_{i=1}^n y_i - \sum_{i=1}^n \beta_0 - \sum_{i=1}^n \beta_1x_i = 0,
\]

and use the facts

\[
\sum_{i=1}^n \beta_0 = n\beta_0,
\qquad
\sum_{i=1}^n \beta_1x_i = \beta_1\sum_{i=1}^n x_i,
\]

to obtain the first normal equation:

\[
\boxed{\sum_{i=1}^n y_i - n\beta_0 - \beta_1\sum_{i=1}^n x_i = 0.}
\]

## 2.2 Partial derivative with respect to \(\beta_1\)

Similarly,

\[
\frac{\partial S}{\partial \beta_1}
=
\sum_{i=1}^n \frac{\partial}{\partial \beta_1}\left(e_i^2\right)
=
\sum_{i=1}^n 2e_i\frac{\partial e_i}{\partial\beta_1}.
\]

Compute the inner derivative:

\[
e_i=y_i-\beta_0-\beta_1x_i
\quad\Longrightarrow\quad
\frac{\partial e_i}{\partial\beta_1}=-x_i.
\]

Therefore,

\[
\frac{\partial S}{\partial \beta_1}
=
\sum_{i=1}^n 2e_i(-x_i)
=
-2\sum_{i=1}^n x_i e_i.
\]

Set to zero:

\[
-2\sum_{i=1}^n x_ie_i = 0
\quad\Longleftrightarrow\quad
\sum_{i=1}^n x_i(y_i-\beta_0-\beta_1x_i)=0.
\]

Expand:

\[
\sum_{i=1}^n x_iy_i - \beta_0\sum_{i=1}^n x_i - \beta_1\sum_{i=1}^n x_i^2 = 0.
\]

This is the second normal equation:

\[
\boxed{\sum_{i=1}^n x_iy_i - \beta_0\sum_{i=1}^n x_i - \beta_1\sum_{i=1}^n x_i^2 = 0.}
\]

# Step 3: Compute the required sums (pure arithmetic)

From the data \((x,y)=(1,2),(2,3),(3,5),(4,4)\):

\[
\sum x_i = 1+2+3+4 = 10,
\]
\[
\sum y_i = 2+3+5+4 = 14,
\]
\[
\sum x_i^2 = 1^2+2^2+3^2+4^2 = 1+4+9+16 = 30,
\]
\[
\sum x_iy_i = (1)(2)+(2)(3)+(3)(5)+(4)(4) = 2+6+15+16 = 39.
\]

Insert these into the normal equations.

## 3.1 First normal equation

\[
\sum y_i - n\beta_0 - \beta_1\sum x_i = 0
\]

\[
14 - 4\beta_0 - 10\beta_1 = 0
\quad\Longleftrightarrow\quad
4\beta_0 + 10\beta_1 = 14.
\]

## 3.2 Second normal equation

\[
\sum x_iy_i - \beta_0\sum x_i - \beta_1\sum x_i^2 = 0
\]

\[
39 - 10\beta_0 - 30\beta_1 = 0
\quad\Longleftrightarrow\quad
10\beta_0 + 30\beta_1 = 39.
\]

So we solve the system

\[
\begin{cases}
4\beta_0 + 10\beta_1 = 14 \quad (1)\\
10\beta_0 + 30\beta_1 = 39 \quad (2)
\end{cases}
\]

# Step 4: Solve by elimination (showing cancellations)

Multiply equation (1) by \(2.5\):

\[
(4\beta_0 + 10\beta_1 = 14)\cdot 2.5
\quad\Longrightarrow\quad
10\beta_0 + 25\beta_1 = 35
\quad (1').
\]

Subtract \((1')\) from \((2)\):

\[
\begin{aligned}
(10\beta_0 + 30\beta_1) - (10\beta_0 + 25\beta_1)
&= 39 - 35\\
\cancel{10\beta_0} + (30-25)\beta_1 - \cancel{10\beta_0}
&= 4\\
5\beta_1 &= 4\\
\beta_1 &= 0.8.
\end{aligned}
\]

Insert \(\beta_1=0.8\) into (1):

\[
4\beta_0 + 10(0.8) = 14
\]

\[
4\beta_0 + 8 = 14
\]

\[
4\beta_0 = 6
\quad\Longrightarrow\quad
\beta_0 = 1.5.
\]

Therefore, the OLS line is

\[
\boxed{\widehat{y}=1.5+0.8x.}
\]

# Step 5: Verify the two first-order conditions numerically

Compute fitted values and residuals:

\[
\widehat{y}_i = 1.5 + 0.8x_i,\qquad e_i=y_i-\widehat{y}_i.
\]

\[
\begin{array}{r|r|r|r}
x_i & y_i & \widehat{y}_i & e_i\\\hline
1 & 2 & 2.3 & -0.3\\
2 & 3 & 3.1 & -0.1\\
3 & 5 & 3.9 & 1.1\\
4 & 4 & 4.7 & -0.7
\end{array}
\]

The optimality conditions derived from partial derivatives are:

1. \(\sum_{i=1}^n e_i = 0\).
2. \(\sum_{i=1}^n x_ie_i = 0\).

Check (1):

\[
(-0.3)+(-0.1)+(1.1)+(-0.7)=0.0.
\]

Check (2):

\[
1(-0.3)+2(-0.1)+3(1.1)+4(-0.7)
=
-0.3-0.2+3.3-2.8=0.0.
\]

Both conditions hold.

# Appendix: Minimal R verification (no external packages)

The calculations below reproduce the sums, solve the normal equations by closed-form
expressions, and verify the two derivative conditions. This is not a replacement for
the algebra above; it is an audit.

```{r}
x <- c(1, 2, 3, 4)
y <- c(2, 3, 5, 4)

n   <- length(x)
sx  <- sum(x)
sy  <- sum(y)
sxx <- sum(x^2)
sxy <- sum(x*y)

# Closed-form from the normal equations:
beta1 <- (n*sxy - sx*sy) / (n*sxx - sx^2)
beta0 <- mean(y) - beta1*mean(x)

beta0
beta1

yhat <- beta0 + beta1*x
e    <- y - yhat

sum_e  <- sum(e)
sum_xe <- sum(x*e)
sse    <- sum(e^2)

c(sum_e = sum_e, sum_xe = sum_xe, sse = sse)
```

```{r}
# Optional: compare to lm (base R; not a contributed package)
fit <- lm(y ~ x)
coef(fit)
```
