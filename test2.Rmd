---
title: "Archive Notes — Formal critique of direct-opposition probes & late-signal learning"
author: "Heini (archive) + GPT synthesis"
date: "2025-12-16"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

# Purpose

This document consolidates two earlier feedback notes into one coherent,
non-redundant archive:

1. **Direct-opposition probes (Packard & McGaugh–style):** why the usual
   "probe arm choice = strategy" inference is not identified and can become
   definitional/circular.
2. **Near-threshold NHST (e.g., `p = .057` interaction):** why dichotomising
   evidence at 0.05 is misleading, and what *design/analysis* choices follow.

It then adds an optional **extension**: modelling *late-onset* learning signals
without post-hoc cherry-picking.

---

# Part I — Direct-opposition probes: the inferential problem in logic

## 1. Observable vs latent variables

Let the **observable** be a probe-trial behaviour:

- \(T\): “training-consistent turn in the probe” (same bodily turn as trained)

Let the intended **latent** (unobserved) cognitive state be:

- \(R\): “animal uses a response strategy”
- \(P\): “animal uses a place strategy”

A common *classification convention* in direct-opposition probes is:

\[
R \leftrightarrow T
\qquad\text{and}\qquad
P \leftrightarrow \neg T.
\]

This makes the "strategy label" essentially a relabelling of the observed turn.

## 2. The definitional tautology

If the experimental paper then claims:

> “The manipulation reduced \(T\), therefore the animal used \(P\).”

the logical step is:

\[
\neg T \Rightarrow P.
\]

But if \(P \equiv \neg T\) is built into the definition, then

\[
(\neg T \Rightarrow P)
\]

is always true (a definitional consequence), and therefore *cannot, by itself,
count as evidence* about the animal's internal computation.

## 3. Minimal countermodel: why \(\neg T \not\Rightarrow P\)

Introduce a second latent variable:

- \(F\): failure to express/execute the trained turn
  (motor/program selection/inhibition, probe novelty, etc.)

Assume only:

\[
F \Rightarrow \neg T,
\]

but **do not** assume \(F \Rightarrow P\).

Then the assignment

\[
F \wedge \neg P \wedge \neg T
\]

is consistent, which proves:

\[
\neg T \not\Rightarrow P
\]

unless one adds the extra premise \(\neg T \Rightarrow P\)—i.e., unless one
imports the definitional/circular step explicitly.

## 4. The hidden assumptions: exhaustiveness and exclusivity

Many readings of the probe behave as if:

- **Exhaustive:** \(S \in \{\text{Place}, \text{Response}\}\)
- **Exclusive:** \(\text{Place} \oplus \text{Response}\) (exactly one true)

Once exclusivity fails (e.g., animals acquire multiple strategies in parallel),
the inference “not response ⇒ place” is invalid even as logic.

## 5. Non-bijective mapping: many minds → one behaviour

The identifiability problem can be stated as:

\[
f: (S, E) \mapsto B
\]

is typically **many-to-one** in realistic environments, where:

- \(S\) is some internal representational/strategic state,
- \(E\) is expression/selection/inhibitory competence,
- \(B\) is the observed probe behaviour.

Thus \(B\) does not uniquely determine \(S\) without independent measurement or
manipulation of \(E\).

## 6. Retention vs acquisition

If a manipulation occurs shortly before the probe, the design estimates:

\[
P(B \mid \text{manipulation at probe}),
\]

not:

\[
P(\text{acquire } R \mid \text{manipulation during learning}).
\]

Those are distinct causal questions, and the probe design alone does not answer
the acquisition question.

---

# Part II — The same argument as a causal DAG

## 1. Variables

- \(M_H, M_S\): manipulations (hippocampus / striatum)
- \(K_P, K_R\): availability/strength of place vs response representations
- \(E\): expression/selection/inhibition competence at probe
- \(T\): observed probe turn / category

A minimal DAG:

\[
M_H \to K_P,\quad M_S \to K_R
\]
\[
K_P \to T,\quad K_R \to T
\]
\[
M_H \to E,\quad M_S \to E,\quad E \to T
\]

Thus \(T\) is influenced by both **representation** and **expression**.
Without measuring \(E\), the effect of \(M\) on \(T\) is underdetermined.

---

# Part III — "Sample size saves random error, not systematic confounds"

Let an estimator be:

\[
\hat{\theta}_n = \theta + b + \varepsilon_n
\]

where:

- \(\theta\) is the causal target,
- \(b\) is systematic bias (confounding, design artifact, forced-choice effects),
- \(\varepsilon_n\) is mean-zero noise.

Then:

\[
\mathbb{E}[\hat{\theta}_n] = \theta + b,
\]

and as \(n \to \infty\),

\[
\varepsilon_n \to 0 \quad\text{(Law of Large Numbers),}
\qquad
b \text{ stays.}
\]

So increasing \(n\) can make one **more certain about the wrong answer** when
\(b \neq 0\).

---

# Part IV — The "almost significant interaction" problem

## 1. What `p = .057` actually licenses

A result like `p = .057` for a group-by-session interaction does **not**
justify “no interaction” in any principled sense. The difference between
0.049 and 0.057 is not a qualitative change in evidence; it is a small change
in a continuous quantity.

The scientifically correct conclusion is typically:

- data are compatible with a range of interaction magnitudes,
- power may be low (especially at small \(n\)),
- designs terminating before asymptote can miss differences in *learning rate*
  or *eventual acquisition*.

## 2. Avoiding sequential p-hacking (without throwing away adaptivity)

If one continues “until it crosses 0.05” after looking, Type I error inflates
unless one uses **planned** sequential designs (alpha spending / group sequential)
or Bayesian stopping rules with transparent priors and decision criteria.

---

# Part V — Extension: modelling *late-onset* learning without cherry-picking

If you expect “flat early, then improvement late,” do not cut the data into
post-hoc chunks. Model an onset explicitly.

## 1. Change-point learning model (binomial trials)

Let \(y_{it}\) be number correct out of \(n=4\) trials per session.
Model:

\[
y_{it} \sim \mathrm{Binomial}(n, p_{it}),
\qquad
\mathrm{logit}(p_{it}) = \eta_{it}.
\]

A late-onset model:

\[
\eta_{it}
= \alpha_{g(i)} + u_i
+ \beta^{\mathrm{pre}}_{g(i)} t
+ \beta^{\mathrm{post}}_{g(i)}(t-\tau_i)_+,
\qquad
(x)_+ = \max(0,x).
\]

- \(\tau_i\) is the subject-specific onset session.
- \(\beta^{\mathrm{post}}>0\) captures “learning starts after onset.”

A smooth approximation (helpful for Bayesian samplers):

\[
(t-\tau)_+ \approx \frac{1}{k}\log\!\left(1+\exp(k(t-\tau))\right).
\]

## 2. Time-to-criterion as survival analysis

Define a pre-specified criterion \(c\) and let \(T_i\) be first session meeting it.
If an animal does not meet criterion by the end, \(T_i\) is right-censored.
Use survival methods (Cox or parametric AFT) to model delays directly.

---

# Part VI — Diagrams

## A. R diagram: causal DAG (DiagrammeR)

```{r dag_r, message=FALSE, warning=FALSE}
# install.packages("DiagrammeR")  # if needed
library(DiagrammeR)

grViz("
digraph G {
  rankdir=LR;
  node [shape=box, style=rounded];

  MH [label='M_H (hippocampus manipulation)'];
  MS [label='M_S (striatum manipulation)'];
  KP [label='K_P (place representation)'];
  KR [label='K_R (response representation)'];
  E  [label='E (expression/selection)'];
  T  [label='T (probe turn / category)'];

  MH -> KP;
  MS -> KR;

  KP -> T;
  KR -> T;

  MH -> E;
  MS -> E;
  E  -> T;
}
")